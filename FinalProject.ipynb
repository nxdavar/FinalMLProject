{
   "cells": [
      {
         "cell_type": "markdown",
         "id": "62e7e44f",
         "metadata": {},
         "source": [
            "# Final ML Project\n",
            "## Daniel Bernal, Raymond Vuong, Rohit Punjani, and Neal Davar \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 166,
         "id": "a208641d",
         "metadata": {},
         "outputs": [],
         "source": [
            "# import statements\n",
            "\n",
            "from sklearn.model_selection import cross_val_score\n",
            "from sklearn.model_selection import GridSearchCV\n",
            "import sklearn.metrics as metrics\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "from sklearn import preprocessing\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import confusion_matrix\n",
            "from sklearn.ensemble import IsolationForest\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from sklearn.tree import DecisionTreeRegressor\n",
            "from sklearn.preprocessing import OneHotEncoder\n",
            "from sklearn.compose import ColumnTransformer\n",
            "from sklearn.neighbors import KNeighborsRegressor\n",
            "from sklearn.ensemble import RandomForestRegressor\n",
            "from sklearn.ensemble import GradientBoostingRegressor\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.model_selection import cross_val_predict\n",
            "from sklearn.ensemble import AdaBoostRegressor\n",
            "from sklearn.model_selection import KFold\n",
            "from sklearn.pipeline import Pipeline\n",
            "from sklearn.decomposition import PCA"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 167,
         "id": "c6d64130",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Constants and other global variables: \n",
            "\n",
            "IL_F_ITERATIONS = 50"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 168,
         "id": "7d9dd670",
         "metadata": {},
         "outputs": [],
         "source": [
            "# clean data by dropping cols like Id, imputing value\n",
            "# depending on the feature, and dropping duplicates:\n",
            "def clean_data(dataset, is_training): \n",
            "    dataset.drop('Id', axis=1, inplace=True)\n",
            "\n",
            "    if is_training: \n",
            "        dataset = dataset.drop('SalePrice', axis=1, inplace=False)\n",
            "    \n",
            "    null_counts = dataset.isnull().sum()\n",
            "    missing_features = null_counts[null_counts > 0]\n",
            "    print(missing_features)    \n",
            "    \n",
            "   \n",
            "    # fillna on the basis of whether MasVnrArea, LotFrontage, or any other categorical feature with na values\n",
            "    dataset['MSZoning'].fillna('None', inplace=True)\n",
            "    dataset['LotFrontage'].fillna(0, inplace=True)\n",
            "    dataset['Alley'].fillna('None', inplace=True)\n",
            "    dataset['Utilities'].fillna('None', inplace=True)\n",
            "    dataset['Exterior1st'].fillna('None', inplace=True)\n",
            "    dataset['Exterior2nd'].fillna('None', inplace=True)\n",
            "    dataset['MasVnrType'].fillna('None', inplace=True)\n",
            "    dataset['MasVnrArea'].fillna(0, inplace=True)\n",
            "    dataset['BsmtQual'].fillna('None', inplace=True)\n",
            "    dataset['BsmtCond'].fillna('None', inplace=True)\n",
            "    dataset['BsmtExposure'].fillna('None', inplace=True)\n",
            "    dataset['BsmtFinType1'].fillna('None', inplace=True)\n",
            "    dataset['BsmtFinSF1'].fillna(0, inplace=True)\n",
            "    dataset['BsmtFinType2'].fillna('None', inplace=True)\n",
            "    dataset['BsmtFinSF2'].fillna(0, inplace=True)\n",
            "    dataset['BsmtUnfSF'].fillna(0, inplace=True)\n",
            "    dataset['TotalBsmtSF'].fillna(0, inplace=True)\n",
            "    dataset['BsmtFullBath'].fillna(0, inplace=True)\n",
            "    dataset['BsmtHalfBath'].fillna(0, inplace=True)\n",
            "    dataset['KitchenQual'].fillna('None', inplace=True)\n",
            "    dataset['Functional'].fillna('None', inplace=True)\n",
            "    dataset['FireplaceQu'].fillna('None', inplace=True)\n",
            "    dataset['GarageType'].fillna('None', inplace=True)\n",
            "    dataset['GarageYrBlt'].fillna(dataset['GarageYrBlt'].mean(), inplace=True)\n",
            "    dataset['GarageFinish'].fillna('None', inplace=True)\n",
            "    dataset['GarageCars'].fillna(0, inplace=True)\n",
            "    dataset['GarageArea'].fillna(0, inplace=True)\n",
            "    dataset['GarageQual'].fillna('None', inplace=True)\n",
            "    dataset['GarageCond'].fillna('None', inplace=True)\n",
            "    dataset['PoolQC'].fillna('None', inplace=True)\n",
            "    dataset['Fence'].fillna('None', inplace=True)\n",
            "    dataset['MiscFeature'].fillna('None', inplace=True)\n",
            "    dataset['SaleType'].fillna('None', inplace=True)\n",
            "\n",
            "    # drop duplicates:\n",
            "    dataset.drop_duplicates(keep=False, inplace=True)\n",
            "\n",
            "    # Changing categorial features to be stored as string\n",
            "    dataset['MSSubClass'] = dataset['MSSubClass'].astype(str)\n",
            "\n",
            "    print(((is_training and \"Training: \") or (not is_training and \"Testing: \")), dataset.describe)\n",
            "\n",
            "    return dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 169,
         "id": "62686913",
         "metadata": {},
         "outputs": [],
         "source": [
            "# on hot encode the data\n",
            "def one_hot_encode(dataset):\n",
            "    ohe_col_list = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n",
            "                    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']   \n",
            "\n",
            "    dataset_dropped = pd.get_dummies(data=dataset, columns=ohe_col_list, drop_first=True)\n",
            "\n",
            "    return dataset_dropped"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 170,
         "id": "5df19066",
         "metadata": {},
         "outputs": [],
         "source": [
            "def explore_data(dataset):\n",
            "    # grab all the numeric features and plot histograms\n",
            "    numeric_feats = dataset.select_dtypes(\n",
            "    include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64'])\n",
            "    for i in numeric_feats.columns:\n",
            "        plt.hist(numeric_feats[i])\n",
            "        plt.title(i)\n",
            "        plt.show()\n",
            "\n",
            "    # plot correlation between features: \n",
            "    corr_mat = dataset.corr()\n",
            "    plt.subplots(figsize=(12, 9))\n",
            "    sns.heatmap(corr_mat, square=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 171,
         "id": "96d7c6e5",
         "metadata": {},
         "outputs": [],
         "source": [
            "# TODO: ASK ANSHUL!!\n",
            "def one_hot_encoder(training, testing):\n",
            "   \n",
            "    ohe = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
            "    labels = training['SalePrice']\n",
            "    training_features = training.drop('SalePrice', axis=1)\n",
            "    ohe.fit(training_features)\n",
            "    transform_training = pd.DataFrame(ohe.transform(training_features).toarray())\n",
            "    print(ohe.get_feature_names_out())\n",
            "    transform_testing = pd.DataFrame(ohe.transform(testing).toarray())\n",
            "\n",
            "    pd.concat([transform_training, labels], axis=1)\n",
            "\n",
            "    print(transform_training)\n",
            "    print(transform_testing)\n",
            "\n",
            "    return transform_training, transform_testing\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 172,
         "id": "1e57f786",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Uses the isolation forest technique to find outliers across 50 iterations\n",
            "# and removes records that are recorded as outliers > 10% of the time\n",
            "def run_isolation_forest(training):\n",
            "    # use isolation forests to find potential outliers:\n",
            "    freq_outlier_map = {} \n",
            "    for i in range(0, IL_F_ITERATIONS):\n",
            "        anomalies = IsolationForest().fit_predict(training, 0.5)\n",
            "        training['anomalies'] = anomalies\n",
            "        outlier_indices = training.loc[training['anomalies'] == -1].index\n",
            "        # add outlier freqs to map\n",
            "        for j in range(0, len(outlier_indices)):\n",
            "            count = 0\n",
            "            if outlier_indices[j] in freq_outlier_map: \n",
            "                count = freq_outlier_map[outlier_indices[j]]\n",
            "            freq_outlier_map[outlier_indices[j]] = count + 1\n",
            "        \n",
            "        inlier_indices = training.loc[training['anomalies'] == 1].index\n",
            "        \n",
            "\n",
            "        \n",
            "    # drop outliers that are detected as anomalies more than 10% of the time\n",
            "    final_outlier_indices = []\n",
            "    print('Total # of Outliers: ')\n",
            "    for outlier_index in freq_outlier_map.keys(): \n",
            "        if freq_outlier_map[outlier_index] > (0.10 * IL_F_ITERATIONS):\n",
            "            final_outlier_indices.append(outlier_index)\n",
            "    print(len(final_outlier_indices))\n",
            "    print('Number of data points before outlier removal: ')\n",
            "    print(len(training))\n",
            "    training.drop(index=final_outlier_indices, inplace=True)\n",
            "    print('Number of data points before after outlier removal: ')\n",
            "    print(len(training))\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 173,
         "id": "8a21aeb6",
         "metadata": {},
         "outputs": [],
         "source": [
            "# function that normalizes the training and testing data:\n",
            "def normalize_data(train, test):\n",
            "    # your code goes here\n",
            "    train_norm = (train - train.min()) / (train.max() - train.min())\n",
            "    test_norm = (test - test.min()) / (test.max() - test.min())\n",
            "    return train_norm, test_norm"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 174,
         "id": "467b2edb",
         "metadata": {},
         "outputs": [],
         "source": [
            "# feature engineering of consolidating bathroom\n",
            "def fe_bathrooms(dataset):\n",
            "    # consolidate bathroom columns into one col:\n",
            "    dataset['TotalBathrooms'] = \\\n",
            "        dataset['FullBath'] + (0.5 * dataset['HalfBath']) + \\\n",
            "        dataset['BsmtFullBath'] + (0.5 * dataset['BsmtHalfBath'])\n",
            "    dataset.drop(['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath'], axis=1)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 175,
         "id": "395765bf",
         "metadata": {},
         "outputs": [],
         "source": [
            "# feature engineering of consolidating porch sq ft: \n",
            "def fe_porch(dataset):\n",
            "    # consolidate porch area into one column\n",
            "    dataset['TotalPorchSF'] = dataset['OpenPorchSF'] + \\\n",
            "        dataset['EnclosedPorch'] + \\\n",
            "        dataset['3SsnPorch'] + dataset['ScreenPorch']\n",
            "\n",
            "    dataset.drop(['OpenPorchSF', 'EnclosedPorch',\n",
            "                  '3SsnPorch', '3SsnPorch'], axis=1)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 176,
         "id": "5306004e",
         "metadata": {},
         "outputs": [],
         "source": [
            "# feature engineering to apply log function\n",
            "def fe_log(dataset, cols):\n",
            "    for col in cols: \n",
            "        dataset[col] = np.log(dataset[col])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 177,
         "id": "30a1cafe",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Run Decision Tree Regression on our Data:\n",
            "def do_decision_tree_regression(training):\n",
            "    labels = training['SalePrice']\n",
            "    features = training.drop('SalePrice', axis=1)\n",
            "    pca = PCA()\n",
            "    dtr = DecisionTreeRegressor()\n",
            "    pipeline = Pipeline(\n",
            "        steps=[('pca', pca), ('dt', dtr)])\n",
            "    param_grid = {'pca__n_components': list(range(5, 20)),\n",
            "                  'dt__max_depth': [5, 10, 15, 20, 30, 40], 'dt__min_samples_leaf': [\n",
            "        5, 10, 15, 20, 30, 40], 'dt__max_features': [5, 10, 15, 30, 40]}\n",
            "\n",
            "    grid = GridSearchCV(pipeline, param_grid, scoring='r2', cv=5)\n",
            "    grid.fit(features, labels)\n",
            "    print(\"DECISION TREE REGRESSION MODEL: \")\n",
            "    print(\"Best number of dimensions: \",\n",
            "          grid.best_params_['pca__n_components'])\n",
            "    print(\"Best parameters: \", grid.best_params_)\n",
            "    # nested cross val\n",
            "    cv = cross_val_score(grid, features, labels, cv=5)\n",
            "    r2 = sum(cv)/cv.size\n",
            "    print(\"R2 with cross val: \", r2)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 178,
         "id": "717907ee",
         "metadata": {},
         "outputs": [],
         "source": [
            "# run adaboost regression on training data\n",
            "def do_adaBoost(training):\n",
            "    labels = training['SalePrice']\n",
            "    features = training.drop('SalePrice', axis=1)\n",
            "    pca = PCA()\n",
            "    ada = AdaBoostRegressor()\n",
            "    pipeline = Pipeline(steps=[('pca', pca), ('ada', ada)])\n",
            "    param_grid = {'pca__n_components': list(range(5, 12)), 'ada__n_estimators': [10, 20, 30,\n",
            "                                    40, 50, 60]}\n",
            "\n",
            "    grid = GridSearchCV(pipeline, param_grid, scoring='r2', cv=5)\n",
            "    grid.fit(features, labels)\n",
            "    print(\"ADA BOOST MODEL: \")\n",
            "    print(\"Best number of dimensions: \",\n",
            "          grid.best_params_['pca__n_components'])\n",
            "    print(\"Best parameters: \", grid.best_params_)\n",
            "    # nested cross val\n",
            "    cv = cross_val_score(grid, features, labels, cv=5)\n",
            "    r2 = sum(cv)/cv.size\n",
            "    print(\"R2 with cross val: \", r2)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 179,
         "id": "34f1f17f",
         "metadata": {},
         "outputs": [],
         "source": [
            "# KNN Regression Model Implementation:\n",
            "#KNN regression\n",
            "def do_KNN_regression(training):\n",
            "    labels = training['SalePrice']\n",
            "    features = training.drop('SalePrice', axis=1)\n",
            "    pca = PCA()\n",
            "    knn = KNeighborsRegressor()\n",
            "    pipeline = Pipeline(steps=[('pca', pca), ('knn', knn)])\n",
            "    param_grid = {'pca__n_components': list(range(5, 12)), 'knn__n_neighbors': [\n",
            "        5, 10, 15, 20, 30, 40]}\n",
            "\n",
            "    grid = GridSearchCV(pipeline, param_grid, scoring='r2', cv=5)\n",
            "    grid.fit(features, labels)\n",
            "    print(\"KNN REGRESSION MODEL: \")\n",
            "    print(\"Best number of dimensions: \",\n",
            "          grid.best_params_['pca__n_components'])\n",
            "    print(\"Best parameters: \", grid.best_params_)\n",
            "    # nested cross val\n",
            "    cv = cross_val_score(grid, features, labels, cv=5)\n",
            "    r2 = sum(cv)/cv.size\n",
            "    print(\"R2 with cross val: \", r2)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "241fed16",
         "metadata": {},
         "outputs": [],
         "source": [
            "# run random forest algorithm on dataset: \n",
            "def do_random_forest_regression(training):\n",
            "  labels = training['SalePrice']\n",
            "  features = training.drop('SalePrice', axis=1)\n",
            "  dt = RandomForestRegressor()\n",
            "  pca = PCA()\n",
            "  pipeline = Pipeline(steps=[('pca', pca), ('dt', dt)])\n",
            "  #innerloop of crossval\n",
            "  param_grid = {'pca__n_components': list(range(5, 12)), \n",
            "                'dt__max_depth': [5, 10, 15, 20, 30, 40], 'dt__min_samples_leaf': [\n",
            "      5, 10, 15, 20, 30, 40], 'dt__max_features': [5, 10, 15, 30, 40]}\n",
            "  grid = GridSearchCV(pipeline,\n",
            "                      param_grid, cv=5, scoring='r2')\n",
            "  grid.fit(features, labels)\n",
            "  print(\"RANDOM FOREST REGRESSION MODEL: \")\n",
            "  print(\"Best parameters: \", grid.best_params_)\n",
            "  cv = cross_val_score(grid, features, labels, cv=5)\n",
            "  r2 = sum(cv)/cv.size\n",
            "  # TODO: Uncomment this: \n",
            "  #   Scores[0,0] = r2\n",
            "  print(\"R2 with cross val: \", r2)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "ab257191",
         "metadata": {},
         "outputs": [],
         "source": [
            "# run gradient boosting regression on dataset\n",
            "def do_gradient_boosting_regression(training):\n",
            "  labels = training['SalePrice']\n",
            "  features = training.drop('SalePrice', axis = 1)\n",
            "  gbr = GradientBoostingRegressor()\n",
            "  pca = PCA()\n",
            "  pipeline = Pipeline(steps=[('pca', pca), ('gbr', gbr)])\n",
            "\n",
            "  #innerloop of crossval\n",
            "  param_grid = {'pca__n_components': list(range(5, 12)), 'gbr__max_depth': [5,10,20,30], 'gbr__min_samples_leaf': [5,10,15,20, 30, 40], 'gbr__max_features': [5, 10, 15]}\n",
            "  grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')\n",
            "  grid.fit(features, labels)\n",
            "  print(\"GRADIENT BOOSTING MODEL: \")\n",
            "  print(\"Best parameters: \", grid.best_params_)\n",
            "\n",
            "  cv = cross_val_score(grid, features, labels, cv = 5)\n",
            "  r2 = sum(cv)/cv.size\n",
            "# TODO: Uncomment this:\n",
            "#   Scores[0,2] = r2\n",
            "  print(\"R2 with cross val\")\n",
            "  print(r2)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 180,
         "id": "41192d54",
         "metadata": {},
         "outputs": [],
         "source": [
            "# run a linear regression model\n",
            "def runLinReg(train_norm):\n",
            "    labels = train_norm['SalePrice']\n",
            "    labels.values.ravel()\n",
            "    features = train_norm.drop('SalePrice', axis=1)\n",
            "    print(features.shape)\n",
            "    print(labels.shape)\n",
            "    print(features.head())\n",
            "    print(labels.head())\n",
            "    pca = PCA()\n",
            "    linReg = LinearRegression() \n",
            "    pipeline = Pipeline(steps=[('pca', pca), ('linreg', linReg)])\n",
            "    scores = cross_val_score(pipeline, features, labels, cv=10)\n",
            "    print(\"Accuracy:\", scores.mean()*100)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 181,
         "id": "530380e8",
         "metadata": {},
         "outputs": [],
         "source": [
            "# run all models on data\n",
            "def run_models(dataset):\n",
            "    print(\"LINEAR REGRESSION: \")\n",
            "    runLinReg(dataset)\n",
            "    print(\"KNN REGRESSION: \")\n",
            "    do_KNN_regression(dataset)\n",
            "    print(\"ADABOOST REGRESSION: \")\n",
            "    do_adaBoost(dataset)\n",
            "    print(\"DECISION TREE REGRESSION: \")\n",
            "    do_decision_tree_regression(dataset)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2b4100f2",
         "metadata": {},
         "outputs": [],
         "source": [
            "# the kaggle dataset testing data does not have labels\n",
            "# so we split the train into train and test:\n",
            "all_data = pd.read_csv(\"train.csv\")\n",
            "training, testing = train_test_split(all_data, test_size=0.50)\n",
            "training_labels = training['SalePrice']\n",
            "\n",
            "\n",
            "# 1. Clean Data\n",
            "training = clean_data(training, True)\n",
            "testing = clean_data(testing, False)\n",
            "\n",
            "\n",
            "\n",
            "#2. Explore Data\n",
            "# TODO: uncomment this line\n",
            "# explore_data(training)\n",
            "\n",
            "#  one hot encode and do an inner join to make sure we have the same shape in test and train\n",
            "ohe_train = one_hot_encode(training)\n",
            "ohe_test = one_hot_encode(testing)\n",
            "final_train, final_test = ohe_train.align(ohe_test, join='inner', axis=1)\n",
            "\n",
            "\n",
            "# 3. Normalization\n",
            "train_norm, test_norm = normalize_data(final_train, final_test)\n",
            "\n",
            "# outlier detection:\n",
            "run_isolation_forest(train_norm)\n",
            "\n",
            "# 4. Feature Engineering: \n",
            "\n",
            "train_norm['SalePrice'] = training_labels\n",
            "# 4. Run Models\n",
            "# run_models(train_norm)\n",
            "do_gradient_boosting_regression(train_norm)\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.9"
      },
      "vscode": {
         "interpreter": {
            "hash": "c66eea1c4c7bb61b575e87dc258962edfd54bd0f6a298796fe44c73819b6ab09"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}

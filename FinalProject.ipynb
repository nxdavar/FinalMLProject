{
   "cells": [
      {
         "cell_type": "markdown",
         "id": "62e7e44f",
         "metadata": {},
         "source": [
            "# Final ML Project\n",
            "## Daniel Bernal, Raymond Vuong, Rohit Punjani, and Neal Davar \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 306,
         "id": "a208641d",
         "metadata": {},
         "outputs": [],
         "source": [
            "# import statements\n",
            "\n",
            "from sklearn.model_selection import cross_val_score\n",
            "from sklearn.model_selection import GridSearchCV\n",
            "import sklearn.metrics as metrics\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "from sklearn import preprocessing\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.metrics import confusion_matrix\n",
            "from sklearn.ensemble import IsolationForest\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "from sklearn.linear_model import LinearRegression\n",
            "from sklearn.tree import DecisionTreeRegressor\n",
            "from sklearn.preprocessing import OneHotEncoder\n",
            "from sklearn.compose import ColumnTransformer\n",
            "from sklearn.neighbors import KNeighborsRegressor\n",
            "from sklearn.ensemble import RandomForestRegressor\n",
            "from sklearn.ensemble import GradientBoostingRegressor\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.model_selection import cross_val_predict\n",
            "from sklearn.ensemble import AdaBoostRegressor\n",
            "from sklearn.model_selection import KFold\n",
            "from sklearn.pipeline import Pipeline\n",
            "from sklearn.decomposition import PCA"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 307,
         "id": "c6d64130",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Constants and other global variables: \n",
            "\n",
            "IL_F_ITERATIONS = 50"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 329,
         "id": "7d9dd670",
         "metadata": {},
         "outputs": [],
         "source": [
            "# clean data by dropping cols like Id, imputing value\n",
            "# depending on the feature, and dropping duplicates:\n",
            "def clean_data(dataset): \n",
            "    dataset.drop('Id', axis=1, inplace=True)\n",
            "    null_counts = dataset.isnull().sum()\n",
            "    missing_features = null_counts[null_counts > 0]\n",
            "    print(missing_features)    \n",
            "    \n",
            "   \n",
            "    # fillna on the basis of whether MasVnrArea, LotFrontage, or any other categorical feature with na values\n",
            "    dataset['MSZoning'].fillna('None', inplace=True)\n",
            "    dataset['LotFrontage'].fillna(0, inplace=True)\n",
            "    dataset['Alley'].fillna('None', inplace=True)\n",
            "    dataset['Utilities'].fillna('None', inplace=True)\n",
            "    dataset['Exterior1st'].fillna('None', inplace=True)\n",
            "    dataset['Exterior2nd'].fillna('None', inplace=True)\n",
            "    dataset['MasVnrType'].fillna('None', inplace=True)\n",
            "    dataset['MasVnrArea'].fillna(0, inplace=True)\n",
            "    dataset['BsmtQual'].fillna('None', inplace=True)\n",
            "    dataset['BsmtCond'].fillna('None', inplace=True)\n",
            "    dataset['BsmtExposure'].fillna('None', inplace=True)\n",
            "    dataset['BsmtFinType1'].fillna('None', inplace=True)\n",
            "    dataset['BsmtFinSF1'].fillna(0, inplace=True)\n",
            "    dataset['BsmtFinType2'].fillna('None', inplace=True)\n",
            "    dataset['BsmtFinSF2'].fillna(0, inplace=True)\n",
            "    dataset['BsmtUnfSF'].fillna(0, inplace=True)\n",
            "    dataset['TotalBsmtSF'].fillna(0, inplace=True)\n",
            "    dataset['BsmtFullBath'].fillna(0, inplace=True)\n",
            "    dataset['BsmtHalfBath'].fillna(0, inplace=True)\n",
            "    dataset['KitchenQual'].fillna('None', inplace=True)\n",
            "    dataset['Functional'].fillna('None', inplace=True)\n",
            "    dataset['FireplaceQu'].fillna('None', inplace=True)\n",
            "    dataset['GarageType'].fillna('None', inplace=True)\n",
            "    dataset['GarageYrBlt'].fillna(dataset['GarageYrBlt'].mean(), inplace=True)\n",
            "    dataset['GarageFinish'].fillna('None', inplace=True)\n",
            "    dataset['GarageCars'].fillna(0, inplace=True)\n",
            "    dataset['GarageArea'].fillna(0, inplace=True)\n",
            "    dataset['GarageQual'].fillna('None', inplace=True)\n",
            "    dataset['GarageCond'].fillna('None', inplace=True)\n",
            "    dataset['PoolQC'].fillna('None', inplace=True)\n",
            "    dataset['Fence'].fillna('None', inplace=True)\n",
            "    dataset['MiscFeature'].fillna('None', inplace=True)\n",
            "    dataset['SaleType'].fillna('None', inplace=True)\n",
            "\n",
            "    # drop duplicates:\n",
            "    dataset.drop_duplicates(keep=False, inplace=True)\n",
            "\n",
            "    # Changing categorial features to be stored as string\n",
            "    dataset['MSSubClass'] = dataset['MSSubClass'].astype(str)\n",
            "\n",
            "\n",
            "    return dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 309,
         "id": "62686913",
         "metadata": {},
         "outputs": [],
         "source": [
            "# on hot encode the data\n",
            "def one_hot_encode(dataset):\n",
            "    ohe_col_list = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n",
            "                    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']   \n",
            "\n",
            "    dataset_dropped = pd.get_dummies(data=dataset, columns=ohe_col_list, drop_first=True)\n",
            "\n",
            "    return dataset_dropped"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 310,
         "id": "5df19066",
         "metadata": {},
         "outputs": [],
         "source": [
            "def explore_data(dataset):\n",
            "    # grab all the numeric features and plot histograms\n",
            "    numeric_feats = dataset.select_dtypes(\n",
            "    include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64'])\n",
            "    for i in numeric_feats.columns:\n",
            "        plt.hist(numeric_feats[i])\n",
            "        plt.title(i)\n",
            "        plt.show()\n",
            "\n",
            "    # plot correlation between features: \n",
            "    corr_mat = dataset.corr()\n",
            "    plt.subplots(figsize=(12, 9))\n",
            "    sns.heatmap(corr_mat, square=True)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 311,
         "id": "1e57f786",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Uses the isolation forest technique to find outliers across 50 iterations\n",
            "# and removes records that are recorded as outliers > 10% of the time\n",
            "def run_isolation_forest(training):\n",
            "    # use isolation forests to find potential outliers:\n",
            "    freq_outlier_map = {} \n",
            "    for i in range(0, IL_F_ITERATIONS):\n",
            "        anomalies = IsolationForest().fit_predict(training, 0.5)\n",
            "        training['anomalies'] = anomalies\n",
            "        outlier_indices = training.loc[training['anomalies'] == -1].index\n",
            "        # add outlier freqs to map\n",
            "        for j in range(0, len(outlier_indices)):\n",
            "            count = 0\n",
            "            if outlier_indices[j] in freq_outlier_map: \n",
            "                count = freq_outlier_map[outlier_indices[j]]\n",
            "            freq_outlier_map[outlier_indices[j]] = count + 1\n",
            "        \n",
            "\n",
            "        \n",
            "    # drop outliers that are detected as anomalies more than 10% of the time\n",
            "    final_outlier_indices = []\n",
            "    print('Total # of Outliers: ')\n",
            "    for outlier_index in freq_outlier_map.keys(): \n",
            "        if freq_outlier_map[outlier_index] > (0.10 * IL_F_ITERATIONS):\n",
            "            final_outlier_indices.append(outlier_index)\n",
            "    print(len(final_outlier_indices))\n",
            "    print('Number of data points before outlier removal: ')\n",
            "    print(len(training))\n",
            "    training.drop(index=final_outlier_indices, inplace=True)\n",
            "    print('Number of data points before after outlier removal: ')\n",
            "    print(len(training))\n",
            "    training.drop('anomalies', axis=1, inplace=True)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 312,
         "id": "8a21aeb6",
         "metadata": {},
         "outputs": [],
         "source": [
            "# function that normalizes the training and testing data:\n",
            "def normalize_data(train, test):\n",
            "    # your code goes here\n",
            "    train_norm = (train - train.min()) / (train.max() - train.min())\n",
            "    test_norm = (test - test.min()) / (test.max() - test.min())\n",
            "    return train_norm, test_norm"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 313,
         "id": "467b2edb",
         "metadata": {},
         "outputs": [],
         "source": [
            "# feature engineering of consolidating bathroom\n",
            "def fe_bathrooms(dataset):\n",
            "    # consolidate bathroom columns into one col:\n",
            "    dataset['TotalBathrooms'] = \\\n",
            "        dataset['FullBath'] + (0.5 * dataset['HalfBath']) + \\\n",
            "        dataset['BsmtFullBath'] + (0.5 * dataset['BsmtHalfBath'])\n",
            "    res = dataset.drop(['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath'], axis=1, inplace=False)\n",
            "    return res"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 314,
         "id": "395765bf",
         "metadata": {},
         "outputs": [],
         "source": [
            "# feature engineering of consolidating porch sq ft: \n",
            "def fe_porch(dataset):\n",
            "    # consolidate porch area into one column\n",
            "    dataset['TotalPorchSF'] = dataset['OpenPorchSF'] + \\\n",
            "        dataset['EnclosedPorch'] + \\\n",
            "        dataset['3SsnPorch'] + dataset['ScreenPorch']\n",
            "\n",
            "    res = dataset.drop(['OpenPorchSF', 'EnclosedPorch',\n",
            "                  '3SsnPorch', '3SsnPorch'], axis=1, inplace=False)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 315,
         "id": "5306004e",
         "metadata": {},
         "outputs": [],
         "source": [
            "# feature engineering to apply log function\n",
            "def fe_log(dataset, cols):\n",
            "    dataset_copy = dataset.copy()\n",
            "    for col in cols: \n",
            "        dataset_copy[col] = np.log(dataset_copy[col])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 316,
         "id": "30a1cafe",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Run Decision Tree Regression on our Data:\n",
            "def do_decision_tree_regression(training, testing):\n",
            "    print('shape of training: ', training.shape)\n",
            "    print('shape of testing: ', testing.shape)\n",
            "    labels = training['SalePrice']\n",
            "    features = training.drop('SalePrice', axis=1, inplace=False)\n",
            "    pca = PCA()\n",
            "    dtr = DecisionTreeRegressor()\n",
            "    pipeline = Pipeline(\n",
            "        steps=[('pca', pca), ('dt', dtr)])\n",
            "    param_grid = {'pca__n_components': list(range(5, 20)),\n",
            "                  'dt__max_depth': [5, 10, 15, 20, 30, 40], 'dt__min_samples_leaf': [\n",
            "        5, 10, 15, 20, 30, 40], 'dt__max_features': [5, 10, 15, 30, 40]}\n",
            "\n",
            "    grid = GridSearchCV(pipeline, param_grid, scoring='r2', cv=5)\n",
            "    grid.fit(features, labels)\n",
            "    print(\"DECISION TREE REGRESSION MODEL: \")\n",
            "    print(\"Best number of dimensions: \",\n",
            "          grid.best_params_['pca__n_components'])\n",
            "    print(\"Best parameters: \", grid.best_params_)\n",
            "    # nested cross val\n",
            "    cv = cross_val_score(grid, features, labels, cv=5)\n",
            "    r2 = sum(cv)/cv.size\n",
            "    print(\"R2 with cross val: \", r2)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 317,
         "id": "717907ee",
         "metadata": {},
         "outputs": [],
         "source": [
            "# run adaboost regression on training data\n",
            "def do_adaBoost(training, testing):\n",
            "    print('shape of training: ', training.shape)\n",
            "    print('shape of testing: ', testing.shape)\n",
            "    labels = training['SalePrice']\n",
            "    features = training.drop('SalePrice', axis=1, inplace=False)\n",
            "    testing_labels = testing['SalePrice']\n",
            "    testing_features = testing.drop('SalePrice', axis=1, inplace=False)\n",
            "    pca = PCA()\n",
            "    ada = AdaBoostRegressor()\n",
            "    pipeline = Pipeline(steps=[('pca', pca), ('ada', ada)])\n",
            "    param_grid = {'pca__n_components': list(range(8,14)), 'ada__n_estimators': [10,11,12,13]}\n",
            "\n",
            "    grid = GridSearchCV(pipeline, param_grid, scoring='r2', cv=5)\n",
            "    grid.fit(features, labels)\n",
            "    print(\"ADA BOOST MODEL: \")\n",
            "    print(\"Best number of dimensions: \",\n",
            "          grid.best_params_['pca__n_components'])\n",
            "    print(\"Best parameters: \", grid.best_params_)\n",
            "    # nested cross val\n",
            "    cv = cross_val_score(grid, features, labels, cv=5)\n",
            "    r2 = sum(cv)/cv.size\n",
            "    print(\"R2 with cross val: \", r2)\n",
            "    test_accuracy = grid.score(testing_features, testing_labels)\n",
            "    print(\"Accuracy after running model on test set: \", test_accuracy)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 318,
         "id": "34f1f17f",
         "metadata": {},
         "outputs": [],
         "source": [
            "# KNN Regression Model Implementation:\n",
            "#KNN regression\n",
            "def do_KNN_regression(training, testing):\n",
            "    print('shape of training: ', training.shape)\n",
            "    print('shape of testing: ', testing.shape)\n",
            "    labels = training['SalePrice']\n",
            "    features = training.drop('SalePrice', axis=1, inplace=False)\n",
            "    testing_labels = testing['SalePrice']\n",
            "    testing_features = testing.drop('SalePrice', axis=1, inplace=False)\n",
            "    pca = PCA()\n",
            "    knn = KNeighborsRegressor()\n",
            "    pipeline = Pipeline(steps=[('pca', pca), ('knn', knn)])\n",
            "    param_grid = {'pca__n_components': list(range(9,14)), 'knn__n_neighbors': [\n",
            "        10,20,30,40]}\n",
            "\n",
            "    grid = GridSearchCV(pipeline, param_grid, scoring='r2', cv=5)\n",
            "    grid.fit(features, labels)\n",
            "    print(\"KNN REGRESSION MODEL: \")\n",
            "    print(\"Best number of dimensions: \",\n",
            "          grid.best_params_['pca__n_components'])\n",
            "    print(\"Best parameters: \", grid.best_params_)\n",
            "    # nested cross val\n",
            "    cv = cross_val_score(grid, features, labels, cv=5)\n",
            "    r2 = sum(cv)/cv.size\n",
            "    print(\"R2 with cross val: \", r2)\n",
            "    test_accuracy = grid.score(testing_features, testing_labels)\n",
            "    print(\"Accuracy after running model on test set: \", test_accuracy)\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 328,
         "id": "241fed16",
         "metadata": {},
         "outputs": [],
         "source": [
            "# run random forest algorithm on dataset: \n",
            "def do_random_forest_regression(training, testing):\n",
            "  labels = training['SalePrice']\n",
            "  features = training.drop('SalePrice', axis=1, inplace=False)\n",
            "  testing_labels = testing['SalePrice']\n",
            "  testing_features = testing.drop('SalePrice', axis=1, inplace=False)\n",
            "  dt = RandomForestRegressor()\n",
            "  pca = PCA()\n",
            "  pipeline = Pipeline(steps=[('pca', pca), ('dt', dt)])\n",
            "  #innerloop of crossval\n",
            "  param_grid = {'pca__n_components': list(range(12, 17)), \n",
            "                'dt__max_depth': [1,3,5,7,9], 'dt__min_samples_leaf': [\n",
            "      5, 10, 15, 20, 30, 40], 'dt__max_features': [1,3,5,10]}\n",
            "  grid = GridSearchCV(pipeline,\n",
            "                      param_grid, cv=5, scoring='r2')\n",
            "  grid.fit(features, labels)\n",
            "  print(\"RANDOM FOREST REGRESSION MODEL: \")\n",
            "  print(\"Best parameters: \", grid.best_params_)\n",
            "  cv = cross_val_score(grid, features, labels, cv=5)\n",
            "  r2 = sum(cv)/cv.size\n",
            "  # TODO: Uncomment this: \n",
            "  #   Scores[0,0] = r2\n",
            "  print(\"R2 with cross val: \", r2)\n",
            "  test_accuracy = grid.score(testing_features, testing_labels)\n",
            "  print(\"Accuracy after running model on test set: \", test_accuracy)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 320,
         "id": "ab257191",
         "metadata": {},
         "outputs": [],
         "source": [
            "# run gradient boosting regression on dataset\n",
            "def do_gradient_boosting_regression(training, testing):\n",
            "  labels = training['SalePrice']\n",
            "  features = training.drop('SalePrice', axis = 1, inplace=False)\n",
            "  testing_labels = testing['SalePrice']\n",
            "  testing_features = testing.drop('SalePrice', axis = 1, inplace=False)\n",
            "  gbr = GradientBoostingRegressor()\n",
            "  pca = PCA()\n",
            "  pipeline = Pipeline(steps=[('pca', pca), ('gbr', gbr)])\n",
            "\n",
            "  #innerloop of crossval\n",
            "  param_grid = {'pca__n_components': list(range(10, 15)), 'gbr__max_depth': [2, 4, 6, 8], 'gbr__min_samples_leaf': [2, 3, 5, 7], 'gbr__max_features': [3, 5, 7, 9]}\n",
            "  grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')\n",
            "  grid.fit(features, labels)\n",
            "  print(\"GRADIENT BOOSTING MODEL: \")\n",
            "  print(\"Best parameters: \", grid.best_params_)\n",
            "  cv = cross_val_score(grid, features, labels, cv = 5)\n",
            "  r2 = sum(cv)/cv.size\n",
            "# TODO: Uncomment this:\n",
            "#   Scores[0,2] = r2\n",
            "  print(\"R2 with cross val\", r2)\n",
            "  test_accuracy = grid.score(testing_features, testing_labels)\n",
            "  print(\"Accuracy after running model on test set: \", test_accuracy)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 321,
         "id": "41192d54",
         "metadata": {},
         "outputs": [],
         "source": [
            "# run a linear regression model\n",
            "def do_LinReg(training, testing):\n",
            "    print('shape of training: ', training.shape)\n",
            "    print('shape of testing: ', testing.shape)\n",
            "    labels = training['SalePrice']\n",
            "    labels.values.ravel()\n",
            "    features = training.drop('SalePrice', axis=1, inplace=False)\n",
            "    testing_labels = testing['SalePrice']\n",
            "    testing_features = testing.drop('SalePrice', axis=1, inplace=False)\n",
            "    print(features.shape)\n",
            "    print(labels.shape)\n",
            "    print(features.head())\n",
            "    print(labels.head())\n",
            "    param_grid = {\n",
            "        'pca__n_components': [9,13,15,20,24],\n",
            "    }\n",
            "    pca = PCA()\n",
            "    linReg = LinearRegression() \n",
            "    pipeline = Pipeline(steps=[('pca', pca), ('linreg', linReg)])\n",
            "    grid = GridSearchCV(pipeline, param_grid, cv=5)\n",
            "    grid.fit(features, labels)\n",
            "    print(\"LINEAR REGRESSION MODEL: \")\n",
            "    print(\"Best parameters: \", grid.best_params_)\n",
            "    # nested cross val\n",
            "    cv = cross_val_score(grid, features, labels, cv=5)\n",
            "    r2 = sum(cv)/cv.size\n",
            "    print(\"R2 with cross val: \", r2)\n",
            "    test_accuracy = grid.score(testing_features, testing_labels)\n",
            "    print(\"Accuracy after running model on test set: \", test_accuracy)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 322,
         "id": "530380e8",
         "metadata": {},
         "outputs": [],
         "source": [
            "# run all models on data\n",
            "def run_models(dataset):\n",
            "    print(\"LINEAR REGRESSION: \")\n",
            "    do_LinReg(dataset)\n",
            "    print(\"KNN REGRESSION: \")\n",
            "    do_KNN_regression(dataset)\n",
            "    print(\"ADABOOST REGRESSION: \")\n",
            "    do_adaBoost(dataset)\n",
            "    print(\"DECISION TREE REGRESSION: \")\n",
            "    do_decision_tree_regression(dataset)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 330,
         "id": "2b4100f2",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "LotFrontage     123\n",
                  "Alley           683\n",
                  "MasVnrType        5\n",
                  "MasVnrArea        5\n",
                  "BsmtQual         19\n",
                  "BsmtCond         19\n",
                  "BsmtExposure     20\n",
                  "BsmtFinType1     19\n",
                  "BsmtFinType2     20\n",
                  "FireplaceQu     329\n",
                  "GarageType       31\n",
                  "GarageYrBlt      31\n",
                  "GarageFinish     31\n",
                  "GarageQual       31\n",
                  "GarageCond       31\n",
                  "PoolQC          725\n",
                  "Fence           584\n",
                  "MiscFeature     702\n",
                  "dtype: int64\n",
                  "LotFrontage     136\n",
                  "Alley           686\n",
                  "MasVnrType        3\n",
                  "MasVnrArea        3\n",
                  "BsmtQual         18\n",
                  "BsmtCond         18\n",
                  "BsmtExposure     18\n",
                  "BsmtFinType1     18\n",
                  "BsmtFinType2     18\n",
                  "Electrical        1\n",
                  "FireplaceQu     361\n",
                  "GarageType       50\n",
                  "GarageYrBlt      50\n",
                  "GarageFinish     50\n",
                  "GarageQual       50\n",
                  "GarageCond       50\n",
                  "PoolQC          728\n",
                  "Fence           595\n",
                  "MiscFeature     704\n",
                  "dtype: int64\n",
                  "Total # of Outliers: \n",
                  "7\n",
                  "Number of data points before outlier removal: \n",
                  "730\n",
                  "Number of data points before after outlier removal: \n",
                  "723\n",
                  "shape of training:  (723, 242)\n",
                  "shape of testing:  (730, 242)\n",
                  "(723, 241)\n",
                  "(723,)\n",
                  "     LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  \\\n",
                  "703     0.242812  0.029587     0.444444     1.000000   0.204380      0.766667   \n",
                  "911     0.000000  0.037327     0.444444     0.714286   0.766423      0.650000   \n",
                  "501     0.239617  0.039744     0.666667     0.428571   0.970803      0.916667   \n",
                  "778     0.191693  0.033186     0.444444     0.428571   0.766423      0.450000   \n",
                  "802     0.201278  0.032247     0.666667     0.428571   0.970803      0.916667   \n",
                  "\n",
                  "     MasVnrArea  BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  ...  SaleType_ConLI  \\\n",
                  "703         0.0    0.000000         0.0   0.154110  ...             0.0   \n",
                  "911         0.0    0.102941         0.0   0.458904  ...             0.0   \n",
                  "501         0.0    0.210084         0.0   0.199486  ...             0.0   \n",
                  "778         0.2    0.000000         0.0   0.000000  ...             0.0   \n",
                  "802         0.0    0.340336         0.0   0.034247  ...             0.0   \n",
                  "\n",
                  "     SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n",
                  "703             0.0           0.0           0.0          1.0   \n",
                  "911             0.0           0.0           0.0          1.0   \n",
                  "501             0.0           0.0           0.0          1.0   \n",
                  "778             0.0           0.0           0.0          1.0   \n",
                  "802             0.0           0.0           0.0          1.0   \n",
                  "\n",
                  "     SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n",
                  "703                    0.0                   0.0                   0.0   \n",
                  "911                    0.0                   0.0                   0.0   \n",
                  "501                    0.0                   0.0                   0.0   \n",
                  "778                    0.0                   0.0                   0.0   \n",
                  "802                    0.0                   0.0                   0.0   \n",
                  "\n",
                  "     SaleCondition_Normal  SaleCondition_Partial  \n",
                  "703                   1.0                    0.0  \n",
                  "911                   1.0                    0.0  \n",
                  "501                   1.0                    0.0  \n",
                  "778                   1.0                    0.0  \n",
                  "802                   1.0                    0.0  \n",
                  "\n",
                  "[5 rows x 241 columns]\n",
                  "703    140000\n",
                  "911    143500\n",
                  "501    226700\n",
                  "778    144000\n",
                  "802    189000\n",
                  "Name: SalePrice, dtype: int64\n",
                  "LINEAR REGRESSION MODEL: \n",
                  "Best parameters:  {'pca__n_components': 24}\n",
                  "R2 with cross val:  0.7035903979802413\n",
                  "Accuracy after running model on test set:  0.6219433212497603\n",
                  "RANDOM FOREST REGRESSION MODEL: \n",
                  "Best parameters:  {'dt__max_depth': 9, 'dt__max_features': 10, 'dt__min_samples_leaf': 5, 'pca__n_components': 14}\n"
               ]
            },
            {
               "ename": "KeyboardInterrupt",
               "evalue": "",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                  "Cell \u001b[0;32mIn[330], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m# 4. Run Models\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# run_models(train_norm)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m# do_gradient_boosting_regression(train_norm, test_norm)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m do_LinReg(train_norm, test_norm)\n\u001b[0;32m---> 39\u001b[0m do_random_forest_regression(train_norm, test_norm)\n",
                  "Cell \u001b[0;32mIn[328], line 19\u001b[0m, in \u001b[0;36mdo_random_forest_regression\u001b[0;34m(training, testing)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRANDOM FOREST REGRESSION MODEL: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest parameters: \u001b[39m\u001b[39m\"\u001b[39m, grid\u001b[39m.\u001b[39mbest_params_)\n\u001b[0;32m---> 19\u001b[0m cv \u001b[39m=\u001b[39m cross_val_score(grid, features, labels, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[1;32m     20\u001b[0m r2 \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(cv)\u001b[39m/\u001b[39mcv\u001b[39m.\u001b[39msize\n\u001b[1;32m     21\u001b[0m \u001b[39m# TODO: Uncomment this: \u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m#   Scores[0,0] = r2\u001b[39;00m\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:509\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    507\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 509\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    510\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    511\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    512\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    513\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    514\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    515\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    516\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    517\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    518\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    519\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    520\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    521\u001b[0m )\n\u001b[1;32m    522\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:267\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 267\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    268\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    269\u001b[0m         clone(estimator),\n\u001b[1;32m    270\u001b[0m         X,\n\u001b[1;32m    271\u001b[0m         y,\n\u001b[1;32m    272\u001b[0m         scorers,\n\u001b[1;32m    273\u001b[0m         train,\n\u001b[1;32m    274\u001b[0m         test,\n\u001b[1;32m    275\u001b[0m         verbose,\n\u001b[1;32m    276\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    277\u001b[0m         fit_params,\n\u001b[1;32m    278\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    279\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    280\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    281\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    284\u001b[0m )\n\u001b[1;32m    286\u001b[0m _warn_about_fit_failures(results, error_score)\n\u001b[1;32m    288\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    679\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 680\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    682\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    684\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    893\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    839\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m         clone(base_estimator),\n\u001b[1;32m    841\u001b[0m         X,\n\u001b[1;32m    842\u001b[0m         y,\n\u001b[1;32m    843\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    844\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    845\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    846\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    847\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    848\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    849\u001b[0m     )\n\u001b[1;32m    850\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    851\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    852\u001b[0m     )\n\u001b[1;32m    853\u001b[0m )\n\u001b[1;32m    855\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    679\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 680\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    682\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    683\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    684\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/pipeline.py:394\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    393\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 394\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    396\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    439\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    441\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    442\u001b[0m ]\n\u001b[1;32m    444\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    451\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    452\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    453\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_joblib_parallel_args(prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    454\u001b[0m )(\n\u001b[1;32m    455\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    456\u001b[0m         t,\n\u001b[1;32m    457\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    458\u001b[0m         X,\n\u001b[1;32m    459\u001b[0m         y,\n\u001b[1;32m    460\u001b[0m         sample_weight,\n\u001b[1;32m    461\u001b[0m         i,\n\u001b[1;32m    462\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[1;32m    463\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    464\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    465\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[1;32m    468\u001b[0m )\n\u001b[1;32m    470\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/utils/fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    183\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[0;32m--> 185\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/tree/_classes.py:1315\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[1;32m   1279\u001b[0m     \u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, X_idx_sorted\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdeprecated\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1280\u001b[0m ):\n\u001b[1;32m   1281\u001b[0m     \u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \n\u001b[1;32m   1283\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1315\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m   1316\u001b[0m         X,\n\u001b[1;32m   1317\u001b[0m         y,\n\u001b[1;32m   1318\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1319\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m   1320\u001b[0m         X_idx_sorted\u001b[39m=\u001b[39;49mX_idx_sorted,\n\u001b[1;32m   1321\u001b[0m     )\n\u001b[1;32m   1322\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/sklearn/tree/_classes.py:392\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree_ \u001b[39m=\u001b[39m Tree(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_)\n\u001b[1;32m    391\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 392\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree_ \u001b[39m=\u001b[39m Tree(\n\u001b[1;32m    393\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_features_in_,\n\u001b[1;32m    394\u001b[0m         \u001b[39m# TODO: tree shouldn't need this in this case\u001b[39;49;00m\n\u001b[1;32m    395\u001b[0m         np\u001b[39m.\u001b[39;49marray([\u001b[39m1\u001b[39;49m] \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_outputs_, dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mintp),\n\u001b[1;32m    396\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_outputs_,\n\u001b[1;32m    397\u001b[0m     )\n\u001b[1;32m    399\u001b[0m \u001b[39m# Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[39mif\u001b[39;00m max_leaf_nodes \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
                  "File \u001b[0;32msklearn/tree/_tree.pyx:601\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree.__cinit__\u001b[0;34m()\u001b[0m\n",
                  "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2793\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[1;32m   2678\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mamax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   2679\u001b[0m          where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   2680\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[39m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2791\u001b[0m \u001b[39m    5\u001b[39;00m\n\u001b[1;32m   2792\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2793\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmaximum, \u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[1;32m   2794\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
                  "File \u001b[0;32m~/opt/anaconda3/envs/machine-learning/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
               ]
            }
         ],
         "source": [
            "# the kaggle dataset testing data does not have labels\n",
            "# so we split the train into train and test:\n",
            "all_data = pd.read_csv(\"train.csv\")\n",
            "training, testing = train_test_split(all_data, test_size=0.50, random_state=11)\n",
            "training_labels = training['SalePrice']\n",
            "testing_labels = testing['SalePrice']\n",
            "\n",
            "# 1. Clean Data\n",
            "training = clean_data(training)\n",
            "testing = clean_data(testing)\n",
            "\n",
            "\n",
            "\n",
            "#2. Explore Data\n",
            "# TODO: uncomment this line\n",
            "# explore_data(training)\n",
            "\n",
            "#  one hot encode and do an inner join to make sure we have the same shape in test and train\n",
            "ohe_train = one_hot_encode(training)\n",
            "ohe_test = one_hot_encode(testing)\n",
            "final_train, final_test = ohe_train.align(ohe_test, join='inner', axis=1)\n",
            "\n",
            "\n",
            "# 3. Normalization\n",
            "train_norm, test_norm = normalize_data(final_train, final_test)\n",
            "\n",
            "# outlier detection:\n",
            "run_isolation_forest(train_norm)\n",
            "\n",
            "# 4. Feature Engineering: \n",
            "\n",
            "train_norm['SalePrice'] = training_labels\n",
            "test_norm['SalePrice'] = testing_labels\n",
            "\n",
            "# 4. Run Models\n",
            "# run_models(train_norm)\n",
            "# do_gradient_boosting_regression(train_norm, test_norm)\n",
            "do_LinReg(train_norm, test_norm)\n",
            "do_random_forest_regression(train_norm, test_norm)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "6d772958",
         "metadata": {},
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f56e9972",
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "43b56851",
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.9"
      },
      "vscode": {
         "interpreter": {
            "hash": "c66eea1c4c7bb61b575e87dc258962edfd54bd0f6a298796fe44c73819b6ab09"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
